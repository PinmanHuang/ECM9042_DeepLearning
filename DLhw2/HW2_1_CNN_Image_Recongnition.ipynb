{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "**problem**: Chapter 6 of the textbook.\n",
    "\n",
    "In this exercise, you will construct a convolutional neural network (CNN) for image recognition using the MNIST dataset. This dataset is a subset of a larger set available from NIST. The MNIST dataset contains a total of 70,000 images of handwritten digits from 0 to 9 with corresponding labels, of which 55,000 examples are in the training set, 5,000 in the validation set, and 10,000 in the test set. The digits have been size-normalized and centered in a fixedsize image, so you donâ€™t need to do any preprocessing to the images.\n",
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Tensorflow and Keras allow us to import and download the MNIST dataset directly from their API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, learning_rate=0.5, debug=False):\n",
    "        \"\"\"\n",
    "        Train NeuralNetwork by fixed learning rate\n",
    "        \"\"\"\n",
    "        self.neuron_layers = []\n",
    "        self.batch_error = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "\n",
    "    def train(self, dataset, mini_batch):\n",
    "        '''\n",
    "        Convolution Layer\n",
    "        '''\n",
    "        conv1 = Convolution()\n",
    "        conv2 = Convolution()\n",
    "        '''\n",
    "        Pooling Layer\n",
    "        '''\n",
    "        output_channels = 12\n",
    "        ksize = 5\n",
    "        stride = 1\n",
    "        eta = np.zeros((BATCH_SIZE, int((28 - ksize + 1) / stride), int((28 - ksize + 1) / stride), output_channels))\n",
    "        output_shape = eta.shape\n",
    "        relu1 = Relu(output_shape)\n",
    "        pool1 = MaxPooling(relu1.output_shape)\n",
    "        \n",
    "        if mini_batch:\n",
    "            batch_num = 0\n",
    "            self.batch_error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            for inputs, outputs in dataset:\n",
    "                if (batch_num%BATCH_SIZE == BATCH_SIZE-1) or (batch_num == len(dataset)):\n",
    "                    self.feed_forward(inputs)\n",
    "                    calculate_batch_error(outputs)\n",
    "                    self.batch_error = self.batch_error/(batch_num%BATCH_SIZE+1)\n",
    "                    feed_backword_batch(outputs)\n",
    "                    self.update_weights(self.learning_rate)\n",
    "                    self.batch_error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                else:\n",
    "                    self.feed_forward(inputs)\n",
    "                    calculate_batch_error(outputs)\n",
    "                batch_num += 1\n",
    "        else:\n",
    "            for inputs, outputs in dataset:\n",
    "                '''\n",
    "                Convolution & Pooling\n",
    "                '''\n",
    "                conv1.conv_forward(inputs, CONV_FILTER)\n",
    "                '''\n",
    "                Fully Connect\n",
    "                '''\n",
    "                self.feed_forward(inputs)\n",
    "                self.feed_backword(outputs)\n",
    "                self.update_weights(self.learning_rate)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        s = inputs\n",
    "        for (i, l) in enumerate(self.neuron_layers):\n",
    "            s = l.feed_forward(s)\n",
    "            if self.debug:\n",
    "                print (\"Layer %s:\" % (i+1), \" output:%s\" % s)\n",
    "        return s\n",
    "\n",
    "    def feed_backword(self, outputs):\n",
    "        layer_num = len(self.neuron_layers)\n",
    "        l = layer_num\n",
    "        previous_deltas = [] \n",
    "        while l != 0:\n",
    "            current_layer = self.neuron_layers[l - 1]\n",
    "            if len(previous_deltas) == 0:\n",
    "                for i in range(len(current_layer.neurons)):\n",
    "                    error = -(outputs[i] - current_layer.neurons[i].output)\n",
    "                    current_layer.neurons[i].calculate_delta(error)\n",
    "            else:\n",
    "                previous_layer = self.neuron_layers[l]\n",
    "                for i in range(len(current_layer.neurons)):\n",
    "                    error = 0\n",
    "                    for j in range(len(previous_deltas)):\n",
    "                        error += previous_deltas[j] * previous_layer.neurons[j].weights[i]\n",
    "                    current_layer.neurons[i].calculate_delta(error)\n",
    "            previous_deltas = current_layer.get_deltas()\n",
    "            if self.debug:\n",
    "                print (\"Layer %s:\" % l, \"deltas:%s\" % previous_deltas)\n",
    "            l -= 1\n",
    "            \n",
    "    def feed_backword_batch(self, outputs):\n",
    "        layer_num = len(self.neuron_layers)\n",
    "        l = layer_num\n",
    "        previous_deltas = [] \n",
    "        # Batch size - fitst output\n",
    "        current_layer = self.neuron_layers[l - 1]\n",
    "        for i in range(len(current_layer.neurons)):\n",
    "            error = self.batch_error\n",
    "            current_layer.neurons[i].calculate_delta(error)\n",
    "        previous_deltas = current_layer.get_deltas()\n",
    "        if self.debug:\n",
    "            print (\"Layer %s:\" % l, \"deltas:%s\" % previous_deltas)\n",
    "        l -= 1\n",
    "        # Batch size\n",
    "        while l != 0:\n",
    "            current_layer = self.neuron_layers[l - 1]\n",
    "            if len(previous_deltas) == 0:\n",
    "                for i in range(len(current_layer.neurons)):\n",
    "                    error = -(outputs[i] - current_layer.neurons[i].output)\n",
    "                    current_layer.neurons[i].calculate_delta(error)\n",
    "            else:\n",
    "                previous_layer = self.neuron_layers[l]\n",
    "                for i in range(len(current_layer.neurons)):\n",
    "                    error = 0\n",
    "                    for j in range(len(previous_deltas)):\n",
    "                        error += previous_deltas[j] * previous_layer.neurons[j].weights[i]\n",
    "                    current_layer.neurons[i].calculate_delta(error)\n",
    "            previous_deltas = current_layer.get_deltas()\n",
    "            if self.debug:\n",
    "                print (\"Layer %s:\" % l, \"deltas:%s\" % previous_deltas)\n",
    "            l -= 1\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for l in self.neuron_layers:\n",
    "            l.update_weights(learning_rate)\n",
    "\n",
    "    def calculate_total_error(self, dataset):\n",
    "        \"\"\"\n",
    "        Return mean squared error of dataset\n",
    "        \"\"\"\n",
    "        total_error = 0\n",
    "        for inputs, outputs in dataset:\n",
    "            actual_outputs = self.feed_forward(inputs)\n",
    "            for i in range(len(outputs)):\n",
    "                total_error += (outputs[i] - actual_outputs[i]) ** 2\n",
    "        return total_error\n",
    "\n",
    "    def calculate_batch_error(self, outputs):\n",
    "        layer_num = len(self.neuron_layers)\n",
    "        l = layer_num\n",
    "        current_layer = self.neuron_layers[l - 1]\n",
    "        for i in range(len(current_layer.neurons)):\n",
    "            self.batch_error[i] -= (outputs[i] - current_layer.neurons[i].output)\n",
    "\n",
    "    def get_output(self, inputs):\n",
    "        return self.feed_forward(inputs)\n",
    "\n",
    "    def add_layer(self, neruon_layer):\n",
    "        self.neuron_layers.append(neruon_layer)\n",
    "\n",
    "    def dump(self):\n",
    "        for (i, l) in enumerate(self.neuron_layers):\n",
    "            print (\"Dump layer: %s\" % (i+1))\n",
    "            l.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer(object):\n",
    "\n",
    "    def __init__(self, input_num, neuron_num, init_weights=[], bias=1):\n",
    "        self.neurons = []\n",
    "        weight_index = 0\n",
    "        for i in range(neuron_num):\n",
    "            n = Neuron(input_num)\n",
    "            for j in range(input_num):\n",
    "                if weight_index < len(init_weights):\n",
    "                    n.weights[j] = init_weights[weight_index]\n",
    "                    weight_index += 1\n",
    "            n.bias = bias\n",
    "            self.neurons.append(n)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for n in self.neurons:\n",
    "            outputs.append(n.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_deltas(self):\n",
    "        return [n.delta for n in self.neurons]\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for n in self.neurons:\n",
    "            n.update_weights(learning_rate)\n",
    "\n",
    "    def dump(self):\n",
    "        for (i, n) in enumerate(self.neurons):\n",
    "            print (\"\\t-Dump neuron: %s\" % (i+1))\n",
    "            n.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron ##\n",
    "### Initialize the Weights of Neural Network ###\n",
    "Using random function to randomly generate the weights.\n",
    "### Calculate Output ###\n",
    "$$z=\\omega_{1}*x_{1}+\\omega_{2}âˆ—x_{2}+\\omega_{3}âˆ—bias$$\n",
    "\n",
    "then using activation function(sigmoid function) to calculate the output\n",
    "\n",
    "$$s=\\frac{1}{1+e^{âˆ’z}}$$\n",
    "\n",
    "then using activation function(relu function) to calculate the output\n",
    "\n",
    "$$s=max(x, 0)$$\n",
    "### Activation Function ###\n",
    "$$s=\\frac{1}{1+e^{âˆ’z}}$$\n",
    "### Calculate Delta ###\n",
    "The error influence:\n",
    "$$\\delta_{L}=(realOutputâˆ’expectOutput)âˆ—gâ€²(z)$$\n",
    "\n",
    "$$g'(z)=oâˆ—(1âˆ’o)$$\n",
    "### Update Weights ###\n",
    "$$\\delta_{L}=\\omega_{L+1}\\delta_{L+1}âˆ—gâ€²(z)$$\n",
    "\n",
    "new weights: $$\\omega_{i}=\\omega_{i}âˆ’\\alphaâˆ—\\deltaâˆ—x_{i}$$\n",
    "\n",
    "new bias: $$\\omega_{bias}=\\omega_{bias}âˆ’\\alphaâˆ—\\delta$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "\n",
    "    def __init__(self, weight_num):\n",
    "        self.weights = []\n",
    "        self.bias = 0\n",
    "        self.output = 0\n",
    "        self.delta = 0\n",
    "        self.inputs = []\n",
    "        for i in range(weight_num):\n",
    "            self.weights.append(random.random())\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if len(inputs) != len(self.weights):\n",
    "            raise Exception(\"Input number not fit weight number\")\n",
    "        self.output = 0\n",
    "        for (i, w) in enumerate(self.weights):\n",
    "            self.output += w * inputs[i]\n",
    "        self.output = self.activation_function(self.output + self.bias)\n",
    "        return self.output\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        \"\"\"Using sigmoid function\"\"\"\n",
    "        #return 1 / (1 + math.exp(-x))\n",
    "        \"\"\"Using relu function\"\"\"\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def calculate_delta(self, error):\n",
    "        \"\"\" Using g' of sigmoid \"\"\"\n",
    "        self.delta = error * self.output * (1 - self.output)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for (i, w) in enumerate(self.weights):\n",
    "            new_w = w - learning_rate * self.delta * self.inputs[i]\n",
    "            self.weights[i] = new_w\n",
    "        self.bias = self.bias - learning_rate * self.delta\n",
    "\n",
    "    def dump(self):\n",
    "        print (\"\\t\\t-- weights:\", self.weights)\n",
    "        print (\"\\t\\t-- bias:\", self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Convolution ##\n",
    "\n",
    "<img src=\"./forwardpass_convolution.PNG\">\n",
    "$$h_{11}=\\omega_{11}*x_{11}+\\omega_{12}âˆ—x_{12}+\\omega_{21}âˆ—x_{21}+\\omega_{22}âˆ—x_{22}$$\n",
    "$$h_{12}=\\omega_{11}*x_{12}+\\omega_{12}âˆ—x_{13}+\\omega_{21}âˆ—x_{22}+\\omega_{22}âˆ—x_{23}$$\n",
    "$$h_{21}=\\omega_{11}*x_{21}+\\omega_{12}âˆ—x_{22}+\\omega_{21}âˆ—x_{31}+\\omega_{22}âˆ—x_{32}$$\n",
    "$$h_{22}=\\omega_{11}*x_{22}+\\omega_{12}âˆ—x_{23}+\\omega_{21}âˆ—x_{32}+\\omega_{22}âˆ—x_{33}$$\n",
    "\n",
    "## Backward Convolution ##\n",
    "\n",
    "<img src=\"./backwardpass_convolution.PNG\">\n",
    "$$\\delta\\omega_{11}=x_{11}\\delta h_{11}+x_{12}\\delta h_{12}+x_{21}\\delta h_{21}+x_{22}\\delta h_{22}$$\n",
    "$$\\delta\\omega_{12}=x_{12}\\delta h_{11}+x_{13}\\delta h_{12}+x_{22}\\delta h_{21}+x_{23}\\delta h_{22}$$\n",
    "$$\\delta\\omega_{21}=x_{21}\\delta h_{11}+x_{22}\\delta h_{12}+x_{31}\\delta h_{21}+x_{32}\\delta h_{22}$$\n",
    "$$\\delta\\omega_{22}=x_{22}\\delta h_{11}+x_{23}\\delta h_{12}+x_{32}\\delta h_{21}+x_{33}\\delta h_{22}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(object):\n",
    "    \n",
    "    def conv_forward(self, X, W):\n",
    "        '''\n",
    "        The forward computation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        X -- output activations of the previous layer, numpy array of shape (n_H_prev, n_W_prev) assuming input channels = 1\n",
    "        W -- Weights, numpy array of size (f, f) assuming number of filters = 1\n",
    "\n",
    "        Returns:\n",
    "        H -- conv output, numpy array of size (n_H, n_W)\n",
    "        cache -- cache of values needed for conv_backward() function\n",
    "        '''\n",
    "\n",
    "        # Retrieving dimensions from X's shape\n",
    "        (n_H_prev, n_W_prev) = X.shape\n",
    "\n",
    "        # Retrieving dimensions from W's shape\n",
    "        (f, f) = W.shape\n",
    "\n",
    "        # Compute the output dimensions assuming no padding and stride = 1\n",
    "        n_H = n_H_prev - f + 1\n",
    "        n_W = n_W_prev - f + 1\n",
    "\n",
    "        # Initialize the output H with zeros\n",
    "        H = np.zeros((n_H, n_W))\n",
    "\n",
    "        # Looping over vertical(h) and horizontal(w) axis of output volume\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                x_slice = X[h:h+f, w:w+f]\n",
    "                H[h,w] = np.sum(x_slice * W)\n",
    "\n",
    "        # Saving information in 'cache' for backprop\n",
    "        cache = (X, W)\n",
    "\n",
    "        return H, cache\n",
    "    \n",
    "    def conv_backward(self, dH, cache):\n",
    "        '''\n",
    "        The backward computation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
    "        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "\n",
    "        Returns:\n",
    "        dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
    "        dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
    "        '''\n",
    "\n",
    "        # Retrieving information from the \"cache\"\n",
    "        (X, W) = cache\n",
    "\n",
    "        # Retrieving dimensions from X's shape\n",
    "        (n_H_prev, n_W_prev) = X.shape\n",
    "\n",
    "        # Retrieving dimensions from W's shape\n",
    "        (f, f) = W.shape\n",
    "\n",
    "        # Retrieving dimensions from dH's shape\n",
    "        (n_H, n_W) = dH.shape\n",
    "\n",
    "        # Initializing dX, dW with the correct shapes\n",
    "        dX = np.zeros(X.shape)\n",
    "        dW = np.zeros(W.shape)\n",
    "\n",
    "        # Looping over vertical(h) and horizontal(w) axis of the output\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                dX[h:h+f, w:w+f] += W * dH(h,w)\n",
    "                dW += X[h:h+f, w:w+f] * dH(h,w)\n",
    "\n",
    "        return dX, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(object):\n",
    "    def __init__(self, shape):\n",
    "        self.eta = np.zeros(shape)\n",
    "        self.x = np.zeros(shape)\n",
    "        self.output_shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def gradient(self, eta):\n",
    "        self.eta = eta\n",
    "        self.eta[self.x<0]=0\n",
    "        return self.eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(object):\n",
    "    def __init__(self, shape, ksize=2, stride=2):\n",
    "        self.input_shape = shape\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.output_channels = shape[-1]\n",
    "        self.index = np.zeros(shape)\n",
    "        self.output_shape = [shape[0], shape[1] / self.stride, shape[2] / self.stride, self.output_channels]\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = np.zeros([x.shape[0], x.shape[1] / self.stride, x.shape[2] / self.stride, self.output_channels])\n",
    "\n",
    "        for b in range(x.shape[0]):\n",
    "            for c in range(self.output_channels):\n",
    "                for i in range(0, x.shape[1], self.stride):\n",
    "                    for j in range(0, x.shape[2], self.stride):\n",
    "                        out[b, i / self.stride, j / self.stride, c] = np.max(\n",
    "                            x[b, i:i + self.ksize, j:j + self.ksize, c])\n",
    "                        index = np.argmax(x[b, i:i + self.ksize, j:j + self.ksize, c])\n",
    "                        self.index[b, i+index/self.stride, j + index % self.stride, c] = 1\n",
    "        return out\n",
    "\n",
    "    def gradient(self, eta):\n",
    "        return np.repeat(np.repeat(eta, self.stride, axis=1), self.stride, axis=2) * self.index\n",
    "    \n",
    "    def pooling_forward(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and Pooling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d12aaaa780>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADqJJREFUeJzt3X+sVPWZx/HPIy3+ACQiFxYteinixh+Jl82EbKLZsKk2sDZBohiIEtYQaQioNfVXMKbGaCLrtghxJV4WIsSWtqG48odZq6YRm9TGEUwR2d0avPIz3EuE1Gq0/Hj2j3tobvHOd4aZM3OG+7xfyc3MnOd873ky8LlnZr4z8zV3F4B4zim6AQDFIPxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4L6RisPNnbsWO/s7GzlIYFQenp6dPjwYatl34bCb2YzJK2UNEzSf7r706n9Ozs7VS6XGzkkgIRSqVTzvnU/7DezYZL+Q9JMSVdLmmdmV9f7+wC0ViPP+adJ+sjdd7v7XyT9XNKsfNoC0GyNhP9SSXsH3N6XbfsbZrbIzMpmVu7r62vgcADy1Ej4B3tR4WufD3b3bncvuXupo6OjgcMByFMj4d8naeKA29+SdKCxdgC0SiPhf1fSFDObZGbDJc2VtCWftgA0W91Tfe5+3MyWSnpN/VN969x9Z26dAWiqhub53f1VSa/m1AuAFuLtvUBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E1dIlujH07N27N1lfuXJlxdqKFSuSY++///5k/b777kvWJ06cmKxHx5kfCIrwA0ERfiAowg8ERfiBoAg/EBThB4JqaJ7fzHokfSbphKTj7l7Koym0j/379yfrU6dOTdaPHj1asWZmybHPPvtssr5+/fpkva+vL1mPLo83+fyzux/O4fcAaCEe9gNBNRp+l/RrM3vPzBbl0RCA1mj0Yf/17n7AzMZJet3M/sfdtw7cIfujsEiSLrvssgYPByAvDZ353f1Adtkr6WVJ0wbZp9vdS+5e6ujoaORwAHJUd/jNbISZjTp1XdJ3JX2QV2MAmquRh/3jJb2cTdd8Q9LP3P2/c+kKQNPVHX533y3puhx7QQE++eSTZH369OnJ+pEjR5L11Fz+6NGjk2PPPffcZL23tzdZ3717d8Xa5Zdfnhw7bNiwZH0oYKoPCIrwA0ERfiAowg8ERfiBoAg/EBRf3T0EHDt2rGKt2lTejBkzkvVqX83diK6urmT9qaeeStZvuOGGZH3KlCkVa93d3cmxCxcuTNaHAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU8/xDwIMPPlix9txzz7WwkzPz1ltvJeuff/55sj579uxkffPmzRVr27dvT46NgDM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFPP9ZoNpn6l966aWKNXdv6NjV5tJvvfXWZP3OO++sWJs4cWJy7FVXXZWsP/zww8n6pk2bKtYavV+GAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxCUVZvvNLN1kr4nqdfdr822jZH0C0mdknok3e7u6bWaJZVKJS+Xyw22PPTs378/Wb/uuvRK6EePHq372HfccUeyvmbNmmT9ww8/TNa3bdtWsTZ37tzk2AsuuCBZrya1zPaIESOSY3fu3JmsV3uPQlFKpZLK5XLlddEHqOXM/6Kk01d2eETSm+4+RdKb2W0AZ5Gq4Xf3rZI+PW3zLEnrs+vrJd2Sc18Amqze5/zj3f2gJGWX4/JrCUArNP0FPzNbZGZlMyv39fU1+3AAalRv+A+Z2QRJyi57K+3o7t3uXnL3UkdHR52HA5C3esO/RdKC7PoCSa/k0w6AVqkafjPbKOl3kv7ezPaZ2UJJT0u6ycz+KOmm7DaAs0jVz/O7+7wKpe/k3MuQdfjw4WR9+fLlyfqRI+m3UIwfP75ibdKkScmxixcvTtaHDx+erHd1dTVUL8oXX3yRrD/zzDPJ+qpVq/JspxC8ww8IivADQRF+ICjCDwRF+IGgCD8QFF/dnYPjx48n6w888ECynvrqbUkaPXp0sv7aa69VrF1xxRXJsceOHUvWo/r444+LbqHpOPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFDM8+dgz549yXq1efxq3nnnnWT9yiuvrPt3n3/++XWPxdmNMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU8fw6WLFmSrFdbBn327NnJeiPz+JGdPHmyYu2cc9LnvWr/ZkMBZ34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrqPL+ZrZP0PUm97n5ttu1xSXdL6st2W+burzaryXawffv2irWtW7cmx5pZsj5nzpy6ekJaai6/2r9JqVTKu522U8uZ/0VJMwbZvsLdu7KfIR18YCiqGn533yrp0xb0AqCFGnnOv9TM/mBm68zsotw6AtAS9YZ/taTJkrokHZT040o7mtkiMyubWbmvr6/SbgBarK7wu/shdz/h7iclrZE0LbFvt7uX3L3U0dFRb58AclZX+M1swoCbsyV9kE87AFqllqm+jZKmSxprZvsk/UjSdDPrkuSSeiR9v4k9AmiCquF393mDbF7bhF7a2pdfflmx9tVXXyXHXnLJJcn6zTffXFdPQ93x48eT9VWrVtX9u2+77bZkfdmyZXX/7rMF7/ADgiL8QFCEHwiK8ANBEX4gKMIPBMVXd7fAeeedl6yPHDmyRZ20l2pTeatXr07WH3rooWS9s7OzYu3RRx9Njh0+fHiyPhRw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoJjnb4H58+cX3UJh9u/fX7G2fPny5Njnn38+Wb/rrruS9TVr1iTr0XHmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgmOevkbvXVZOkF198MVl/7LHH6mmpLWzcuDFZv+eeeyrWjhw5khx77733JusrVqxI1pHGmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqo6z29mEyVtkPR3kk5K6nb3lWY2RtIvJHVK6pF0u7unJ27PYmZWV02S9u3bl6w/8cQTyfrChQuT9VGjRlWs7dy5Mzn2hRdeSNbffvvtZL2npydZnzx5csXa3Llzk2OrzfOjMbWc+Y9L+qG7XyXpHyUtMbOrJT0i6U13nyLpzew2gLNE1fC7+0F335Zd/0zSLkmXSpolaX2223pJtzSrSQD5O6Pn/GbWKWmqpN9LGu/uB6X+PxCSxuXdHIDmqTn8ZjZS0q8k/cDd/3QG4xaZWdnMyn19ffX0CKAJagq/mX1T/cH/qbtvzjYfMrMJWX2CpN7Bxrp7t7uX3L3U0dGRR88AclA1/Nb/UvZaSbvc/ScDSlskLciuL5D0Sv7tAWiWWj7Se72k+ZJ2mNn72bZlkp6W9EszWyhpj6Q5zWnx7HfixIlkvdpU39q1a5P1MWPGVKzt2LEjObZRM2fOTNZnzJhRsbZ06dK828EZqBp+d/+tpEoT2d/Jtx0ArcI7/ICgCD8QFOEHgiL8QFCEHwiK8ANB8dXdNbrmmmsq1m688cbk2DfeeKOhY1f7SHBqGexqxo1LfyRj8eLFyfrZ/LXj0XHmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgmOev0YUXXlixtmnTpuTYDRs2JOvN/IrqJ598Mlm/++67k/WLL744z3bQRjjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQ5u4tO1ipVPJyudyy4wHRlEollcvl9JrxGc78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU1fCb2UQz+42Z7TKznWZ2X7b9cTPbb2bvZz//0vx2AeSlli/zOC7ph+6+zcxGSXrPzF7Paivc/d+b1x6AZqkafnc/KOlgdv0zM9sl6dJmNwaguc7oOb+ZdUqaKun32aalZvYHM1tnZhdVGLPIzMpmVu7r62uoWQD5qTn8ZjZS0q8k/cDd/yRptaTJkrrU/8jgx4ONc/dudy+5e6mjoyOHlgHkoabwm9k31R/8n7r7Zkly90PufsLdT0paI2la89oEkLdaXu03SWsl7XL3nwzYPmHAbrMlfZB/ewCapZZX+6+XNF/SDjN7P9u2TNI8M+uS5JJ6JH2/KR0CaIpaXu3/raTBPh/8av7tAGgV3uEHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IqqVLdJtZn6RPBmwaK+lwyxo4M+3aW7v2JdFbvfLs7XJ3r+n78loa/q8d3Kzs7qXCGkho197atS+J3upVVG887AeCIvxAUEWHv7vg46e0a2/t2pdEb/UqpLdCn/MDKE7RZ34ABSkk/GY2w8z+18w+MrNHiuihEjPrMbMd2crD5YJ7WWdmvWb2wYBtY8zsdTP7Y3Y56DJpBfXWFis3J1aWLvS+a7cVr1v+sN/Mhkn6P0k3Sdon6V1J89z9w5Y2UoGZ9UgquXvhc8Jm9k+S/ixpg7tfm237N0mfuvvT2R/Oi9z94Tbp7XFJfy565eZsQZkJA1eWlnSLpH9Vgfddoq/bVcD9VsSZf5qkj9x9t7v/RdLPJc0qoI+25+5bJX162uZZktZn19er/z9Py1XorS24+0F335Zd/0zSqZWlC73vEn0VoojwXypp74Db+9ReS367pF+b2XtmtqjoZgYxPls2/dTy6eMK7ud0VVdubqXTVpZum/uunhWv81ZE+Adb/aedphyud/d/kDRT0pLs4S1qU9PKza0yyMrSbaHeFa/zVkT490maOOD2tyQdKKCPQbn7geyyV9LLar/Vhw+dWiQ1u+wtuJ+/aqeVmwdbWVptcN+104rXRYT/XUlTzGySmQ2XNFfSlgL6+BozG5G9ECMzGyHpu2q/1Ye3SFqQXV8g6ZUCe/kb7bJyc6WVpVXwfdduK14X8iafbCrjWUnDJK1z96da3sQgzOzb6j/bS/2LmP6syN7MbKOk6er/1NchST+S9F+SfinpMkl7JM1x95a/8Faht+nqf+j615WbTz3HbnFvN0h6W9IOSSezzcvU//y6sPsu0dc8FXC/8Q4/ICje4QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKj/BxmeJtv9WSKzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d12aa2f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 1\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  3.13052033e+00,  6.37206930e+01,\n",
       "          2.30370564e+02,  4.22761103e+02,  4.02521577e+02,\n",
       "          1.98215959e+02,  4.56959683e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          2.94637208e+00,  1.76552282e+02,  6.88208565e+02,\n",
       "          1.21442532e+03,  1.22316145e+03,  1.05428532e+03,\n",
       "          7.23841731e+02,  2.83537604e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  3.31466859e+00,\n",
       "          1.75686786e+02,  8.00935442e+02,  1.10994684e+03,\n",
       "          1.38989920e+03,  1.61599606e+03,  1.70377087e+03,\n",
       "          1.10770435e+03,  6.46556639e+02,  5.84417375e+01,\n",
       "          5.48351620e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.13827516e-01,  1.42635211e+01,  2.04067403e+02,\n",
       "          7.85831083e+02,  1.02853477e+03,  1.41295569e+03,\n",
       "          1.56774050e+03,  1.49520704e+03,  1.37055802e+03,\n",
       "          1.25136907e+03,  1.07148654e+03,  4.40203689e+02,\n",
       "          1.19530808e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.18007842e+01,  3.24685843e+02,  9.13961754e+02,\n",
       "          1.07209452e+03,  1.40788125e+03,  1.59060022e+03,\n",
       "          1.51479617e+03,  1.21499800e+03,  1.25227146e+03,\n",
       "          1.35833417e+03,  1.41923993e+03,  8.66098328e+02,\n",
       "          3.20548836e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  3.13052033e+00,\n",
       "          4.13723420e+02,  9.04788427e+02,  1.22860241e+03,\n",
       "          1.44547210e+03,  1.51418405e+03,  1.40606986e+03,\n",
       "          1.53373563e+03,  1.37665921e+03,  1.00943641e+03,\n",
       "          8.63402805e+02,  1.31043847e+03,  1.10107349e+03,\n",
       "          4.70521807e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  2.94637208e+00,  1.76552282e+02,\n",
       "          6.95575378e+02,  1.08430609e+03,  1.51902193e+03,\n",
       "          1.38244551e+03,  9.38558773e+02,  1.03005192e+03,\n",
       "          1.05736552e+03,  9.25922448e+02,  5.72170468e+02,\n",
       "          5.88957006e+02,  1.00310031e+03,  1.20632169e+03,\n",
       "          6.27761101e+02,  4.56959683e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          2.33254456e+00,  1.54952166e+02,  7.20713285e+02,\n",
       "          9.72237021e+02,  1.40542175e+03,  1.36301725e+03,\n",
       "          8.71349660e+02,  6.55748186e+02,  2.68460822e+02,\n",
       "          1.95487771e+02,  4.29046799e+02,  1.45126643e+02,\n",
       "          3.82624635e+02,  9.75877240e+02,  1.37607360e+03,\n",
       "          8.88746243e+02,  2.17735410e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  4.29679261e-01,\n",
       "          1.01155023e+02,  5.44020222e+02,  9.27188009e+02,\n",
       "          1.26367543e+03,  1.27418701e+03,  8.37807534e+02,\n",
       "          6.41543806e+02,  2.04960864e+02, -7.75327027e+01,\n",
       "          9.36692319e+01,  1.02618330e+02,  1.60784343e+01,\n",
       "          3.11133386e+02,  8.58221512e+02,  1.61748560e+03,\n",
       "          1.08250451e+03,  4.37394021e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  1.87555938e+01,\n",
       "          4.29499482e+02,  8.50498579e+02,  1.26459793e+03,\n",
       "          9.43570112e+02,  5.56839572e+02,  3.59712722e+02,\n",
       "          2.41342935e+02,  1.01799417e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.11133386e+02,  8.49121198e+02,  1.57086539e+03,\n",
       "          1.16197118e+03,  5.65605818e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  1.29309518e+02,\n",
       "          6.31707249e+02,  1.07171462e+03,  1.07099937e+03,\n",
       "          7.83862492e+02,  2.82612661e+02,  3.50741654e+01,\n",
       "          2.78483860e+01,  2.14379123e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.11256152e+02,  8.51298692e+02,  1.54347828e+03,\n",
       "          1.17819448e+03,  5.89488929e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.66508912e+00,  4.69425890e+02,\n",
       "          7.87342863e+02,  1.23740600e+03,  1.05012699e+03,\n",
       "          5.79595065e+02,  4.82353028e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.15492466e+02,  8.52497128e+02,  1.54310130e+03,\n",
       "          1.12843086e+03,  5.46959574e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.70862541e+02,  4.86825826e+02,\n",
       "          9.94816782e+02,  1.27905218e+03,  9.24987038e+02,\n",
       "          3.18262253e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.29679261e-01,  1.56930607e+01,\n",
       "          4.58346036e+02,  9.68387992e+02,  1.35802311e+03,\n",
       "          8.95405155e+02,  3.59171014e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.13631294e+02,  4.92855150e+02,\n",
       "          1.04407725e+03,  1.19050869e+03,  7.72823122e+02,\n",
       "          1.19221007e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.29679261e-01,  3.07043067e+01,  4.58896381e+02,\n",
       "          7.85895895e+02,  1.07557796e+03,  9.20650800e+02,\n",
       "          5.84326354e+02,  1.29379971e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.04530980e+02,  4.91063669e+02,\n",
       "          1.05601885e+03,  1.13141707e+03,  6.19038211e+02,\n",
       "          1.91409932e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.94637208e+00,\n",
       "          7.61715997e+01,  5.16544785e+02,  9.31845477e+02,\n",
       "          1.04354279e+03,  8.20294725e+02,  6.13627353e+02,\n",
       "          2.43162491e+02,  9.18767672e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.04592362e+02,  4.92183108e+02,\n",
       "          8.99975383e+02,  1.19226956e+03,  5.70491788e+02,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  6.99763368e+00,  2.39845332e+02,\n",
       "          7.52793255e+02,  1.07821740e+03,  9.64111335e+02,\n",
       "          5.81415849e+02,  4.59642684e+02,  2.05997060e+02,\n",
       "          5.43604206e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.06710519e+02,  4.93841404e+02,\n",
       "          1.15464613e+03,  1.23785182e+03,  7.97230387e+02,\n",
       "          1.85998996e+02,  7.97692758e+01,  1.27364517e+02,\n",
       "          2.79828162e+02,  6.64737504e+02,  1.02539794e+03,\n",
       "          1.17979617e+03,  1.06731797e+03,  7.18606534e+02,\n",
       "          5.43184737e+02,  1.32455673e+02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.03519834e+02,  4.90527567e+02,\n",
       "          1.12555125e+03,  1.61753240e+03,  1.20857248e+03,\n",
       "          7.73180769e+02,  7.19107633e+02,  9.56220222e+02,\n",
       "          1.22307712e+03,  1.31801732e+03,  1.19777351e+03,\n",
       "          1.09728940e+03,  7.77911718e+02,  5.27738214e+02,\n",
       "          1.99004996e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.01032163e+02,  4.27501214e+02,\n",
       "          9.98411328e+02,  1.66492854e+03,  1.71116411e+03,\n",
       "          1.56168531e+03,  1.46233489e+03,  1.38166380e+03,\n",
       "          1.44522476e+03,  1.26901616e+03,  9.37445325e+02,\n",
       "          4.70945401e+02,  3.67486478e+02,  1.54464297e+02,\n",
       "          4.28758247e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00, -2.49203099e+01,  2.37582333e+02,\n",
       "          6.70060530e+02,  1.19799629e+03,  1.51978120e+03,\n",
       "          1.63575256e+03,  1.56786278e+03,  1.40722853e+03,\n",
       "          1.01863801e+03,  5.46955394e+02,  3.48723109e+02,\n",
       "          2.11821102e+02,  9.95331644e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00, -2.83120906e+01, -1.33428352e+02,\n",
       "          1.55098793e+02,  6.76546891e+02,  9.30364152e+02,\n",
       "          1.09627701e+03,  8.55379292e+02,  5.79414866e+02,\n",
       "          3.67910671e+02,  2.96810081e+02,  1.11017760e+02,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00, -2.52786523e+01,\n",
       "         -1.17550601e+02, -1.74862194e+02, -3.81069960e+01,\n",
       "          5.83185201e+01,  1.70846332e+02,  2.22510005e+02,\n",
       "          1.25531828e+02,  2.83286699e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]]),\n",
       " (array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60,\n",
       "          224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252,\n",
       "          252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253,\n",
       "          253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252,\n",
       "          179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,\n",
       "           84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,\n",
       "           28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,\n",
       "            0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,\n",
       "            0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85,\n",
       "          178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252,\n",
       "          252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252,\n",
       "          233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,\n",
       "           37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]], dtype=uint8),\n",
       "  array([[ 0.76563973,  0.47504397, -1.01114609],\n",
       "         [ 1.33877429,  0.59819524,  2.17953957],\n",
       "         [ 0.91391937,  1.0580556 ,  0.06138275]])))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONV_FILTER = np.random.standard_normal((3, 3))\n",
    "conv1 = Convolution()\n",
    "conv2 = Convolution()\n",
    "conv1.conv_forward(x_train[image_index], CONV_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_channels = 12\n",
    "ksize = 5\n",
    "stride = 1\n",
    "eta = np.zeros((BATCH_SIZE, int((28 - ksize + 1) / stride), int((28 - ksize + 1) / stride), output_channels))\n",
    "output_shape = eta.shape\n",
    "\n",
    "relu1 = Relu(output_shape)\n",
    "pool1 = MaxPooling(relu1.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "hidden_layer_1 = NeuronLayer(input_num=15, neuron_num=10, bias=1)\n",
    "hidden_layer_2 = NeuronLayer(input_num=10, neuron_num=10, bias=1)\n",
    "output_layer = NeuronLayer(input_num=10, neuron_num=1, bias=1)\n",
    "nn.add_layer(hidden_layer_1)\n",
    "nn.add_layer(hidden_layer_2)\n",
    "nn.add_layer(output_layer)\n",
    "# nn.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-073ec7c5df1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_total_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tracking = []\n",
    "BATCH_SIZE = 64\n",
    "for i in range(1):\n",
    "    nn.train(dataset, 0)\n",
    "    tracking.append(nn.calculate_total_error(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img = x_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE].reshape([BATCH_SIZE, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
